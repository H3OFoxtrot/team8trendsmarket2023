{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f6eb528-bba0-43b8-9bc1-e27f2d728809",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"tmfall2023patientdata\"\n",
    "storage_account_key = \"b9CrNE7Gq8QiiqC6YX9c2F09fK5cubb+IeNUw35aJFoPhOhBBnGDpvDI7rk/CyeHrDIxWXAsa8I/+ASt8b4Szg==\"\n",
    "container = \"patientdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb31810-c3c2-4c20-8c75-0f771b2f973a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import signal\n",
    "from scipy.integrate import trapz\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "import  pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType,StructField\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1557598-9383-4139-835b-7510a725f4a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebfaaa99-fa18-4704-864a-7a49f84ad25d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window,expr,mean,stddev,min,max,skewness,median,lag,abs,sum,count\n",
    "\n",
    "def dataimport(filepath, sensortype, Starttime = 'NaN',Endtime = 'NaN',windows='5min'): \n",
    "\n",
    "    if sensortype == 'Glucose':\n",
    "        data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/\"+filepath).toDF('Index','Time','Event Type','Event Subtype',\t'Patient Info','Device Info','Source Device ID','Var','Insulin Value (u)','Carb Value (grams)','Duration (hh:mm:ss)','Glucose Rate of Change (mg/dL/min)',\t'Transmitter Time (Long Integer)')\n",
    "        columns_to_drop = ['Index','Event Type','Event Subtype',\t'Patient Info','Device Info','Source Device ID','Insulin Value (u)','Carb Value (grams)','Duration (hh:mm:ss)','Glucose Rate of Change (mg/dL/min)',\t'Transmitter Time (Long Integer)']\n",
    "        data = data.select([col for col in data.columns if col not in columns_to_drop])\n",
    "\n",
    "    elif sensortype == 'Food':\n",
    "        data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/\"+filepath).toDF('date','times','Time','time_end','logged_food','amount',\t'unit',\t'searched_food','calorie','total_carb','dietary_fiber','sugar','protein','total_fat')\n",
    "        columns_to_drop = ['date','times','time_end','logged_food','amount',\t'unit','searched_food','dietary_fiber','total_fat']\n",
    "        data = data.select([col for col in data.columns if col not in columns_to_drop])\n",
    "\n",
    "    elif sensortype == 'ACC':\n",
    "        data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/\"+filepath).toDF('Time','x','y','z')\n",
    "        data = data.withColumn(\"Var\", (data[\"x\"]**2 + data[\"y\"]**2 + data[\"z\"]**2)**(-2))\n",
    "        columns_to_drop = ['x', 'y', 'z']\n",
    "        data = data.select([col for col in data.columns if col not in columns_to_drop])\n",
    " \n",
    "    else:\n",
    "        data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/\"+filepath).toDF('Time','Var')\n",
    "        \n",
    "    data = data.withColumn(\"Time\", to_timestamp(data[\"Time\"], \"yyyy-MM-dd HH:mm:ss.SSS\"))\n",
    "    \n",
    "    if Starttime != 'NaN':\n",
    "        VarData = data.filter(data[\"Time\"] >= Starttime)\n",
    "        if Endttime != 'NaN':\n",
    "            VarData = VarData.filter((VarData[\"Time\"] <= Endtime))\n",
    "    else:\n",
    "        VarData = data\n",
    "\n",
    "    return VarData\n",
    "\n",
    "\n",
    "def ACC_feature(ACC_data,sensortype = 'ACC'):\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    Data_1= ACC_data.withColumn(\"Sensor_Mean\", ACC_data[\"Var\"].cast(\"double\")) \\\n",
    "    .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "        .agg(\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.25)\").alias(sensortype+'_Q1G'),\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.75)\").alias(sensortype+'_Q3G'),\n",
    "        mean(\"Sensor_Mean\").alias(sensortype+\"mean\"),\n",
    "        stddev(\"Sensor_Mean\").alias(sensortype+\"dev\"),\n",
    "        min(\"Sensor_Mean\").alias(sensortype+\"min\"),\n",
    "        max(\"Sensor_Mean\").alias(sensortype+\"max\"),\n",
    "        skewness(\"Sensor_Mean\").alias(sensortype+\"_skewness\")\n",
    "    )\n",
    "        \n",
    "    ACC_data = ACC_data.withColumn('Time', F.to_timestamp('Time'))\n",
    "    window_ = Window.orderBy(\"Time\")\n",
    "    ACC_data = ACC_data.withColumn(\"lag\", F.lag(\"Time\").over(window_))\n",
    "    ACC_data = ACC_data.withColumn(\"time_diff\", F.col(\"Time\").cast(\"long\") - F.col(\"lag\").cast(\"long\"))\n",
    "\n",
    "    window_spec = Window.orderBy('time_diff').rangeBetween(-7200, 0)\n",
    "    ACC_data = ACC_data.withColumn('ACC_mean_2hr', mean('Var').over(window_spec))\n",
    "    ACC_data = ACC_data.withColumn('ACC_max_2hr', max('Var').over(window_spec))\n",
    "    \n",
    "    Data_2 = ACC_data.select(['Time','ACC_mean_2hr','ACC_max_2hr'])\n",
    "\n",
    "    Data_2 = Data_2.withColumnRenamed('Time','Time_')\n",
    "    \n",
    "    Data = Data_1.join(Data_2,Data_1['Time']['start'] == Data_2['Time_'], how = 'inner')\n",
    "\n",
    "    #Data = Data.select([''])\n",
    "\n",
    "    print((sensortype + ' Import and Resample Complete'))\n",
    "    \n",
    "    return(Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5ff9f308-7425-44a4-a9f9-a05d6e72fa5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC Import and Resample Complete\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Time</th><th>ACC_Q1G</th><th>ACC_Q3G</th><th>ACCmean</th><th>ACCdev</th><th>ACCmin</th><th>ACCmax</th><th>ACC_skewness</th><th>Time_</th><th>ACC_mean_2hr</th><th>ACC_max_2hr</th></tr></thead><tbody><tr><td>List(2020-02-13T15:30:00.000+0000, 2020-02-13T15:35:00.000+0000)</td><td>6.120788621992582E-8</td><td>6.608468091011823E-8</td><td>6.580386701626236E-8</td><td>3.884151720395423E-8</td><td>2.5433045865057125E-9</td><td>1.6394490795313143E-6</td><td>21.894884265345883</td><td>2020-02-13T15:30:00.000+0000</td><td>1.7233871955669065E-7</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "2020-02-13T15:30:00.000+0000",
          "2020-02-13T15:35:00.000+0000"
         ],
         6.120788621992582E-8,
         6.608468091011823E-8,
         6.580386701626236E-8,
         3.884151720395423E-8,
         2.5433045865057125E-9,
         1.6394490795313143E-6,
         21.894884265345883,
         "2020-02-13T15:30:00.000+0000",
         1.7233871955669065E-7,
         1.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"spark.timeWindow\":true}",
         "name": "Time",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"start\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"end\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "ACC_Q1G",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACC_Q3G",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACCmean",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACCdev",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACCmin",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACCmax",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACC_skewness",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Time_",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "ACC_mean_2hr",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACC_max_2hr",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filepath = \"ACC_001.csv\"\n",
    "# sensortype = 'ACC'\n",
    "# ACC_data = dataimport(filepath, sensortype)\n",
    "# ACC = ACC_feature(ACC_data)\n",
    "# ACC.limit(1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef97f1b9-6e62-4e93-af24-14cbc5f37251",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, lit\n",
    "\n",
    "def exercisepts_spark(df):\n",
    "    # Define a window specification to calculate running averages\n",
    "    windowSpec = Window.orderBy(\"Time\").rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "    # Calculate running averages for acc and hr\n",
    "    df = df.withColumn(\"avg_acc\", avg(\"ACC\").over(windowSpec))\n",
    "    df = df.withColumn(\"avg_hr\", avg(\"HR\").over(windowSpec))\n",
    "\n",
    "    # Define a condition for activity bouts\n",
    "    activity_condition = (col(\"ACC\") > col(\"avg_acc\")) & (col(\"HR\") > col(\"avg_hr\"))\n",
    "\n",
    "    # Apply the condition and create a new column for activity bouts\n",
    "    df = df.withColumn(\"Activity Bouts\", F.when(activity_condition, lit(1)).otherwise(lit(0)))\n",
    "\n",
    "    # Count the number of activity bouts\n",
    "    countbouts = df.filter(col(\"Activity Bouts\") == 1).count()\n",
    "\n",
    "    # Select relevant columns to return\n",
    "    returndf = df.select(\"Time\", \"Activity Bouts\")\n",
    "\n",
    "    return returndf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5ec154-8833-4593-b027-1e43cbef238b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def HRV_feature(IBI_data, ibimultiplier = 1000,sensortype='HRV'):\n",
    "    \"\"\"\n",
    "        computes Heart Rate Variability metrics\n",
    "        Args:\n",
    "            time (pandas.DataFrame column or pandas series): time column\n",
    "            IBI (pandas.DataFrame column or pandas series): column with inter beat intervals\n",
    "            ibimultiplier (IntegerType): defualt = 1000; transforms IBI to milliseconds. If data is already in ms, set as 1\n",
    "        Returns:\n",
    "            maxHRV (FloatType): maximum HRV\n",
    "            minHRV (FloatType): minimum HRV\n",
    "            meanHRV (FloatType): mean HRV\n",
    "            medianHRV(FloatType): median HRV\n",
    "    \"\"\"\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    #IBI_data['Var'] = IBI_data['Var']*ibimultiplier\n",
    "\n",
    "    Data = IBI_data.withColumn(\"Sensor_Mean\", IBI_data[\"Var\"].cast(\"double\")) \\\n",
    "    .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "        .agg(\n",
    "        (mean(\"Sensor_Mean\")*ibimultiplier).alias(sensortype+\"mean\"),\n",
    "        (median(\"Sensor_Mean\")*ibimultiplier).alias(sensortype+\"dev\"),\n",
    "        (min(\"Sensor_Mean\")*ibimultiplier).alias(sensortype+\"min\"),\n",
    "        (max(\"Sensor_Mean\")*ibimultiplier).alias(sensortype+\"max\")\n",
    "    )\n",
    "    \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1673960d-ee51-4d72-adfa-8effe10bc1dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def TEMP_feature(TEMP_data,sensortype = 'TEMP'):\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    Data = TEMP_data.withColumn(\"Sensor_Mean\", col(\"Var\").cast(\"double\")) \\\n",
    "    .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "        .agg(\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.25)\").alias(sensortype+'_Q1G'),\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.75)\").alias(sensortype+'_Q3G'),\n",
    "        mean(\"Sensor_Mean\").alias(sensortype+\"mean\"),\n",
    "        stddev(\"Sensor_Mean\").alias(sensortype+\"dev\"),\n",
    "        min(\"Sensor_Mean\").alias(sensortype+\"min\"),\n",
    "        max(\"Sensor_Mean\").alias(sensortype+\"max\"),\n",
    "        skewness(\"Sensor_Mean\").alias(sensortype+\"_skewness\")\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67fa7d17-ae02-41b9-b806-6253cb744396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def HR_feature(HR_data,sensortype='HR'):\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    HR_data = HR_data.withColumn('Time', F.to_timestamp(col('Time'), 'yyyy/M/d h:mm:ss a'))\n",
    "\n",
    "    Data = HR_data.withColumn(\"Sensor_Mean\", col(\"Var\").cast(\"double\")) \\\n",
    "    .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "        .agg(\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.25)\").alias(sensortype+'_Q1G'),\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.75)\").alias(sensortype+'_Q3G'),\n",
    "        mean(\"Sensor_Mean\").alias(sensortype+\"mean\"),\n",
    "        stddev(\"Sensor_Mean\").alias(sensortype+\"dev\"),\n",
    "        min(\"Sensor_Mean\").alias(sensortype+\"min\"),\n",
    "        max(\"Sensor_Mean\").alias(sensortype+\"max\"),\n",
    "        skewness(\"Sensor_Mean\").alias(sensortype+\"_skewness\")\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return(Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2465bc8e-f34e-47b1-b848-3d911f7695f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filepath = 'HR_001.csv'\n",
    "# sensortype = 'HR'\n",
    "# HR_data = dataimport(filepath, sensortype)\n",
    "# HR = HR_feature(HR_data)\n",
    "# HR.limit(1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6c4a95-0f00-4a21-81ec-bb0b33ea9e25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def EDA_feature(EDA_data,sensortype='EDA'):\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    Data = EDA_data.withColumn(\"Sensor_Mean\", col(\"Var\").cast(\"double\")) \\\n",
    "    .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "        .agg(\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.25)\").alias(sensortype+'_Q1G'),\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.75)\").alias(sensortype+'_Q3G'),\n",
    "        mean(\"Sensor_Mean\").alias(sensortype+\"mean\"),\n",
    "        stddev(\"Sensor_Mean\").alias(sensortype+\"dev\"),\n",
    "        min(\"Sensor_Mean\").alias(sensortype+\"min\"),\n",
    "        max(\"Sensor_Mean\").alias(sensortype+\"max\"),\n",
    "        skewness(\"Sensor_Mean\").alias(sensortype+\"_skewness\")\n",
    "    )\n",
    "    \n",
    "    return(Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "be4eced4-94d4-40f2-baec-9198c8f05999",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filepath = 'EDA_001.csv'\n",
    "# sensortype = 'EDA'\n",
    "# EDA_data = dataimport(filepath, sensortype)\n",
    "# EDA = EDA_feature(EDA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2156912f-778c-4fa5-94cc-24f7438aaea2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Time</th><th>EDA_Q1G</th><th>EDA_Q3G</th><th>EDAmean</th><th>EDAdev</th><th>EDAmin</th><th>EDAmax</th><th>EDA_skewness</th></tr></thead><tbody><tr><td>List(2020-02-13T16:05:00.000+0000, 2020-02-13T16:10:00.000+0000)</td><td>0.143759</td><td>0.34106</td><td>0.23949569749999966</td><td>0.1432057896137001</td><td>0.0</td><td>0.677743</td><td>0.7531171686652001</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "2020-02-13T16:05:00.000+0000",
          "2020-02-13T16:10:00.000+0000"
         ],
         0.143759,
         0.34106,
         0.23949569749999966,
         0.1432057896137001,
         0.0,
         0.677743,
         0.7531171686652001
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"spark.timeWindow\":true}",
         "name": "Time",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"start\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"end\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "EDA_Q1G",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "EDA_Q3G",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "EDAmean",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "EDAdev",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "EDAmin",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "EDAmax",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "EDA_skewness",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EDA.limit(1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371024c1-16f0-4a57-9b46-c45f56f1e3e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def NNx_feature(IBI_data, ibimultiplier=1000, x=50):\n",
    "    \"\"\"\n",
    "        computes Heart Rate Variability metrics NNx and pNNx\n",
    "        Args:\n",
    "            time (pandas.DataFrame column or pandas series): time column\n",
    "            IBI (pandas.DataFrame column or pandas series): column with inter beat intervals\n",
    "            ibimultiplier (IntegerType): defualt = 1000; transforms IBI to milliseconds. If data is already in ms, set as 1\n",
    "            x (IntegerType): default = 50; set the number of times successive heartbeat intervals exceed 'x' ms\n",
    "        Returns:\n",
    "            NNx (FloatType): the number of times successive heartbeat intervals exceed x ms\n",
    "            pNNx (FloatType): the proportion of NNx divided by the total number of NN (R-R) intervals. \n",
    "    \"\"\"\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    window_spec = Window().orderBy(\"Time\")\n",
    "    IBI_data = IBI_data.withColumn(\"diff_IBI_mean\", abs(col(\"Var\") - lag(\"Var\").over(window_spec)))\n",
    "\n",
    "    Data = IBI_data.withColumn(\"Sensor_Mean\", col(\"Var\").cast(\"double\")) \\\n",
    "     .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "         .agg(\n",
    "         (mean(\"Sensor_Mean\")*1000).alias(\"IBI_mean\"),\n",
    "         sum((col(\"diff_IBI_mean\") > 0.05).cast(\"int\")).alias('NN50'),\n",
    "         (sum((col(\"diff_IBI_mean\") > 0.05).cast(\"int\"))/count(col('diff_IBI_mean')) * 100).alias('pNN50')\n",
    "\n",
    "     )\n",
    "    \n",
    "    \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64095884-4519-41eb-8278-cc0454aab72b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def Food_feature(Food):\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    Food = Food.groupby('Time').agg(sum(\"calorie\").alias(\"calorie\"),\n",
    "        sum(\"protein\").alias(\"protein\"),\n",
    "        sum(\"sugar\").alias(\"sugar\"),\n",
    "        sum(\"total_carb\").alias(\"total_carb\"))\n",
    "\n",
    "    Food = Food.withColumn('Time', F.to_timestamp('Time'))\n",
    "    window = Window.orderBy(\"Time\")\n",
    "    Food = Food.withColumn(\"lag\", F.lag(\"Time\").over(window))\n",
    "    Food = Food.withColumn(\"time_diff\", F.col(\"Time\").cast(\"long\") - F.col(\"lag\").cast(\"long\"))\n",
    "\n",
    "    for window in ['2H','8H','24H']:\n",
    "        window_spec = Window.orderBy('time_diff').rangeBetween(-3600*int(window[0]), 0)\n",
    "        # Calculate rolling sum for each column\n",
    "        Food = Food.withColumn(f'Calories_{window}', sum('calorie').over(window_spec))\n",
    "        Food = Food.withColumn(f'Protein_{window}', sum('protein').over(window_spec))\n",
    "        Food = Food.withColumn(f'Sugar_{window}', sum('sugar').over(window_spec))\n",
    "        Food = Food.withColumn(f'Carbs_{window}', sum('total_carb').over(window_spec))\n",
    "    Food = Food.select(['Time','Calories_2H','Protein_2H','Sugar_2H','Carbs_2H','Calories_8H','Protein_8H','Sugar_8H','Carbs_8H','Calories_24H','Protein_24H','Sugar_24H','Carbs_24H'])\n",
    "    \n",
    "    return Food\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ef023b-055c-45e7-a204-80f9b9a10876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def PeaksEDA(eda):\n",
    "\n",
    "    eda_pd = eda.toPandas()\n",
    "    \n",
    "    EDAy = eda_pd['Var'].to_numpy()\n",
    "    EDAx = eda_pd['Time'].to_numpy()\n",
    "    peaks, _ = find_peaks(EDAy, height=0, distance=4, prominence=0.3)\n",
    "    \n",
    "    peaks_x = [eda_pd['Time'][i] for i in peaks]\n",
    "    \n",
    "    peakdf_pd = pd.DataFrame({\"Time\": peaks_x, \"Peak\": [1] * len(peaks_x)})\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"Time\", TimestampType(), True),\n",
    "        StructField(\"Peak\", DoubleType(), True),\n",
    "    ])\n",
    "    peakdf = spark.createDataFrame(peakdf_pd, schema=schema)\n",
    "\n",
    "    return peakdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5145bad5-9f85-4b32-9cea-ded1d6f06d22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def Calculate_PeakEda(data):\n",
    "\n",
    "    data = data.withColumn('Time', F.to_timestamp('Time'))\n",
    "    window = Window.orderBy(\"Time\")\n",
    "    data = data.withColumn(\"lag\", F.lag(\"Time\").over(window))\n",
    "    data = data.withColumn(\"time_diff\", F.col(\"Time\").cast(\"long\") - F.col(\"lag\").cast(\"long\"))\n",
    "\n",
    "    windowSpec = Window.orderBy(\"time_diff\").rangeBetween(-3600 * 2, 0)\n",
    "\n",
    "    data = data.withColumn(\"PeakEDA2hr_sum\", sum(\"Peak\").over(windowSpec))\n",
    "    data = data.withColumn(\"PeakEDA2hr_mean\", F.avg(\"Peak\").over(windowSpec))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901002b9-9310-4ee5-8390-ba76faa78965",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def Glucose_feature(Dexcom):\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    Data = Dexcom.withColumn(\"Sensor_Mean\", col(\"Var\").cast(\"double\")) \\\n",
    "     .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "         .agg(\n",
    "         (mean(\"Sensor_Mean\")).alias(\"Glucose_mean\")\n",
    "\n",
    "     )\n",
    "         \n",
    "    # Convert the timestamp to time and extract hour and minute\n",
    "    Data = Data.withColumn('Timefrommidnight', F.date_format(F.col('Time')['start'], 'HH:mm:ss'))\n",
    "    Data = Data.withColumn('minfrommid', (F.hour(col('Time')['start']) * 60 + F.minute(col('Time')['start']) + F.second(col('Time')['start']) / 60).cast('int'))\n",
    "    Data = Data.withColumn('hourfrommid', (F.col('minfrommid') / 60).cast('int'))\n",
    "    \n",
    "    Data = Data.select(['Time','Glucose_mean','minfrommid','hourfrommid'])\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74cf75f7-f282-4378-bb1c-0554f474c1a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_patient(id):\n",
    "\n",
    "    postfix = '00'+str(id) if id < 10 else '0'+str(id)\n",
    "\n",
    "    ## ACC\n",
    "    filepath = \"ACC_\"+postfix+\".csv\"\n",
    "    sensortype = 'ACC'\n",
    "    ACC_data = dataimport(filepath, sensortype)\n",
    "    ACC = ACC_feature(ACC_data)\n",
    "\n",
    "    ## IBI\n",
    "    filepath = 'IBI_'+postfix+\".csv\"\n",
    "    sensortype = 'IBI'\n",
    "    IBI_data = dataimport(filepath, sensortype)\n",
    "    HRV = HRV_feature(IBI_data)\n",
    "\n",
    "    ## TEMP\n",
    "    filepath = 'TEMP_'+postfix+\".csv\"\n",
    "    sensortype = 'TEMP'\n",
    "    TEMP_data = dataimport(filepath, sensortype)\n",
    "    TEMP = TEMP_feature(TEMP_data)\n",
    "\n",
    "    ## HR\n",
    "    filepath = 'HR_'+postfix+\".csv\"\n",
    "    sensortype = 'HR'\n",
    "    HR_data = dataimport(filepath, sensortype)\n",
    "    HR = HR_feature(HR_data)\n",
    "\n",
    "    ## EDA\n",
    "    filepath = 'EDA_'+postfix+\".csv\"\n",
    "    sensortype = 'EDA'\n",
    "    EDA_data = dataimport(filepath, sensortype)\n",
    "    EDA = EDA_feature(EDA_data)\n",
    "    \n",
    "    ## NN\n",
    "    NNx = NNx_feature(IBI_data)\n",
    "\n",
    "    ## Food\n",
    "    filepath = 'Food_Log_'+postfix+\".csv\"\n",
    "    sensortype = 'Food'\n",
    "    Food_data = dataimport(filepath, sensortype)\n",
    "    Food = Food_feature(Food_data)\n",
    "\n",
    "    ## PeakEDA\n",
    "    Peak_EDA = Calculate_PeakEda(PeaksEDA(EDA_data))\n",
    "    Peak_EDA = Peak_EDA.select(['Time','Peak','PeakEDA2hr_sum','PeakEDA2hr_mean'])\n",
    "\n",
    "    # ## Activity\n",
    "    # ACC_data = ACC_data.withColumnRenamed('Var','ACC')\n",
    "    # HR_data = HR_data.withColumnRenamed('Var','HR')\n",
    "    # data_test = ACC_data.join(HR_data,on = 'Time',how = 'inner')\n",
    "    # Activity = exercisepts_spark(data_test)\n",
    "\n",
    "    ## Glucose\n",
    "    filepath = 'Dexcom_'+postfix+\".csv\"\n",
    "    sensortype = 'Glucose'\n",
    "    glucose_data = dataimport(filepath, sensortype)\n",
    "    Glucose = Glucose_feature(glucose_data)\n",
    "\n",
    "    ## Join the features\n",
    "    result_df = ACC.join(HR, on=\"Time\", how=\"full_outer\")\\\n",
    "    .join(NNx,on=\"Time\", how=\"full_outer\")\\\n",
    "        .join(EDA,on=\"Time\", how=\"full_outer\")\\\n",
    "            .join(HRV,on=\"Time\", how=\"full_outer\")\\\n",
    "                .join(Glucose,on == 'Time', how=\"inner\")\\\n",
    "                    .join(TEMP,on=\"Time\", how=\"full_outer\")\n",
    "\n",
    "    result_df = result_df.withColumn('Time', col('Time')['start'])\n",
    "\n",
    "    result_df = result_df.withColumn('PatientID', lit(id))\n",
    "\n",
    "    final_data = result_df.join(Food,on = 'Time',how = 'left').join(Peak_EDA,on = 'Time',how = 'left')\n",
    "    \n",
    "    ## Add a column - patientid\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fadfa5c-ccbe-4066-a2e7-c99375e401c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC Import and Resample Complete\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2106309826890345>:10\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m demo_df \u001B[38;5;241m=\u001B[39m demo_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHbA1c\u001B[39m\u001B[38;5;124m\"\u001B[39m,demo_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHbA1c\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
       "\u001B[1;32m      8\u001B[0m demo_df \u001B[38;5;241m=\u001B[39m demo_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mID\u001B[39m\u001B[38;5;124m\"\u001B[39m,demo_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mID\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \n",
       "\u001B[0;32m---> 10\u001B[0m final_data \u001B[38;5;241m=\u001B[39m merge_patient(\u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m     11\u001B[0m final_data\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n",
       "\n",
       "File \u001B[0;32m<command-2106309826890344>:65\u001B[0m, in \u001B[0;36mmerge_patient\u001B[0;34m(id)\u001B[0m\n",
       "\u001B[1;32m     58\u001B[0m Glucose \u001B[38;5;241m=\u001B[39m Glucose_feature(glucose_data)\n",
       "\u001B[1;32m     60\u001B[0m \u001B[38;5;66;03m## Join the features\u001B[39;00m\n",
       "\u001B[1;32m     61\u001B[0m result_df \u001B[38;5;241m=\u001B[39m ACC\u001B[38;5;241m.\u001B[39mjoin(HR, on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull_outer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m     62\u001B[0m \u001B[38;5;241m.\u001B[39mjoin(NNx,on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull_outer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m     63\u001B[0m     \u001B[38;5;241m.\u001B[39mjoin(EDA,on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull_outer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m     64\u001B[0m         \u001B[38;5;241m.\u001B[39mjoin(HRV,on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull_outer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[0;32m---> 65\u001B[0m             \u001B[38;5;241m.\u001B[39mjoin(Glucose,\u001B[43mon\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m'\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m     66\u001B[0m                 \u001B[38;5;241m.\u001B[39mjoin(TEMP,on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull_outer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     68\u001B[0m result_df \u001B[38;5;241m=\u001B[39m result_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m'\u001B[39m, col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
       "\u001B[1;32m     70\u001B[0m result_df \u001B[38;5;241m=\u001B[39m result_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPatientID\u001B[39m\u001B[38;5;124m'\u001B[39m, lit(\u001B[38;5;28mid\u001B[39m))\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'on' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2106309826890345>:10\u001B[0m\n\u001B[1;32m      7\u001B[0m demo_df \u001B[38;5;241m=\u001B[39m demo_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHbA1c\u001B[39m\u001B[38;5;124m\"\u001B[39m,demo_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHbA1c\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      8\u001B[0m demo_df \u001B[38;5;241m=\u001B[39m demo_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mID\u001B[39m\u001B[38;5;124m\"\u001B[39m,demo_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mID\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \n\u001B[0;32m---> 10\u001B[0m final_data \u001B[38;5;241m=\u001B[39m merge_patient(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     11\u001B[0m final_data\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n\nFile \u001B[0;32m<command-2106309826890344>:65\u001B[0m, in \u001B[0;36mmerge_patient\u001B[0;34m(id)\u001B[0m\n\u001B[1;32m     58\u001B[0m Glucose \u001B[38;5;241m=\u001B[39m Glucose_feature(glucose_data)\n\u001B[1;32m     60\u001B[0m \u001B[38;5;66;03m## Join the features\u001B[39;00m\n\u001B[1;32m     61\u001B[0m result_df \u001B[38;5;241m=\u001B[39m ACC\u001B[38;5;241m.\u001B[39mjoin(HR, on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull_outer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m     62\u001B[0m \u001B[38;5;241m.\u001B[39mjoin(NNx,on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull_outer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;241m.\u001B[39mjoin(EDA,on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull_outer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;241m.\u001B[39mjoin(HRV,on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull_outer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[0;32m---> 65\u001B[0m             \u001B[38;5;241m.\u001B[39mjoin(Glucose,\u001B[43mon\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m'\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m     66\u001B[0m                 \u001B[38;5;241m.\u001B[39mjoin(TEMP,on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull_outer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     68\u001B[0m result_df \u001B[38;5;241m=\u001B[39m result_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m'\u001B[39m, col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     70\u001B[0m result_df \u001B[38;5;241m=\u001B[39m result_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPatientID\u001B[39m\u001B[38;5;124m'\u001B[39m, lit(\u001B[38;5;28mid\u001B[39m))\n\n\u001B[0;31mNameError\u001B[0m: name 'on' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'on' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Write data into different files as per the patient id\n",
    "patient_id = list(range(2,17))\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "demo_df = spark.read.option('header','true').csv(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/Demographics.csv\")\n",
    "demo_df = demo_df.withColumn(\"Gender\",when(demo_df['Gender']=='Male',1).otherwise(0))\n",
    "demo_df = demo_df.withColumn(\"HbA1c\",demo_df['HbA1c'])\n",
    "demo_df = demo_df.withColumn(\"ID\",demo_df['ID'].cast(\"string\")) \n",
    "\n",
    "final_data = merge_patient(1)\n",
    "final_data.limit(1).display()\n",
    "for id in patient_id:\n",
    "    final_data_ = merge_patient(id)\n",
    "    final_data = final_data.union(final_data_)\n",
    "    final_data.limit(1).display()\n",
    "\n",
    "final_data_ = final_data.join(demo_df,data.PatientID == demo_df.ID,how = 'left')\n",
    "\n",
    "final_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").option(\"delimiter\", \",\").csv(f\"wasbs://{container}@{storage_account_name}.#blob.core.windows.net/feature_eng/final_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83aa56d0-12de-40c4-a87e-61bc2e7d92a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#final_data.limit(1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b0c808-d4f6-44b7-83b1-593fcace802d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ID</th><th>Gender</th><th>HbA1c</th></tr></thead><tbody><tr><td>13</td><td>MALE</td><td>5.7</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "13",
         "MALE",
         "5.7"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "HbA1c",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.option('header','true').csv(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/Demographics.csv\").limit(1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6625651-f0e0-471a-81d9-23625c15d63b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: [FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/feature_eng.csv/', name='feature_eng.csv/', size=0, modificationTime=1701323408000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient001.csv/', name='patient001.csv/', size=0, modificationTime=1701451342000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient002.csv/', name='patient002.csv/', size=0, modificationTime=1701451604000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient003.csv/', name='patient003.csv/', size=0, modificationTime=1701451770000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient004.csv/', name='patient004.csv/', size=0, modificationTime=1701451945000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient005.csv/', name='patient005.csv/', size=0, modificationTime=1701452194000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient006.csv/', name='patient006.csv/', size=0, modificationTime=1701452377000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient007.csv/', name='patient007.csv/', size=0, modificationTime=1701461535000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient008.csv/', name='patient008.csv/', size=0, modificationTime=1701461786000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient009.csv/', name='patient009.csv/', size=0, modificationTime=1701462225000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient010.csv/', name='patient010.csv/', size=0, modificationTime=1701462461000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient011.csv/', name='patient011.csv/', size=0, modificationTime=1701462991000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient012.csv/', name='patient012.csv/', size=0, modificationTime=1701463199000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient013.csv/', name='patient013.csv/', size=0, modificationTime=1701463438000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient014.csv/', name='patient014.csv/', size=0, modificationTime=1701463638000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient015.csv/', name='patient015.csv/', size=0, modificationTime=1701463809000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient016.csv/', name='patient016.csv/', size=0, modificationTime=1701464015000),\n FileInfo(path='wasbs://patientdata@tmfall2023patientdata.blob.core.windows.net/feature_eng/patient1.csv/', name='patient1.csv/', size=0, modificationTime=1701406033000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/feature_eng/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f9191e9-1a1e-407e-897a-16cfb93bfbb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2023-11-28 20_52_48 (1)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
